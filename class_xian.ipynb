{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4ef9d4-c571-47c0-8137-99ddc6dfe5d7",
   "metadata": {},
   "source": [
    "# Mindcon 食物分类和西安旅游分类通用notebook\n",
    "\n",
    "## 1.下载数据到notebook环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcc0eee-1f5d-4362-b2c5-897e0de75f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Listing OBS: 1000\n",
      "INFO:root:Listing OBS: 2000\n",
      "INFO:root:Listing OBS: 3000\n",
      "INFO:root:Listing OBS: 4000\n",
      "INFO:root:Listing OBS: 5000\n",
      "INFO:root:Listing OBS: 6000\n",
      "INFO:root:Listing OBS: 7000\n",
      "INFO:root:Listing OBS: 1000\n",
      "INFO:root:Listing OBS: 2000\n",
      "INFO:root:Listing OBS: 3000\n",
      "INFO:root:Listing OBS: 4000\n",
      "INFO:root:Listing OBS: 5000\n",
      "INFO:root:Listing OBS: 6000\n",
      "INFO:root:Listing OBS: 7000\n",
      "INFO:root:pid: None.\t1000/7462\n",
      "INFO:root:pid: None.\t2000/7462\n",
      "INFO:root:pid: None.\t3000/7462\n",
      "INFO:root:pid: None.\t4000/7462\n",
      "INFO:root:pid: None.\t5000/7462\n",
      "INFO:root:pid: None.\t6000/7462\n",
      "INFO:root:pid: None.\t7000/7462\n"
     ]
    }
   ],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy_parallel('obs://mindcon00001/raw/train_data', '/home/ma-user/work/raw')\n",
    "mox.file.copy_parallel('obs://mindcon00001/mindcon_xian_travel/test', '/home/ma-user/work/test')\n",
    "mox.file.copy('obs://mindcon00001/raw/label_id_name.json','/home/ma-user/work/label_id_name.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ccab2-cfdd-4dca-bad1-4f54b8a55db0",
   "metadata": {},
   "source": [
    "## 2.打开终端，下载预训练模型\n",
    "```\n",
    "mkdir ckpt\n",
    "cd ckpt\n",
    "wget https://download.mindspore.cn/vision/classification/vit_b_16_224.ckpt\n",
    "pwd\n",
    "```\n",
    "## 3.处理数据方便MindSpore读取。\n",
    "\n",
    "处理成标签为文件夹，每个文件夹内含有对应图片\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa73948-23cc-4db6-aaf5-521d89c56718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3cfe651-1078-4fc6-8918-03afd9871a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './raw/'\n",
    "target_path = './data/train/'\n",
    "\n",
    "files = os.listdir(path)\n",
    "for file in files:\n",
    "    if file.endswith('.txt'):\n",
    "        with open(path + file,'r') as f:\n",
    "            line = f.read()\n",
    "            image, label = line.split(', ')\n",
    "            if not os.path.exists(target_path + label):\n",
    "                os.makedirs(target_path + label)\n",
    "            shutil.copyfile(path + image, target_path + label + '/' + image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2cf3cc-28d1-44c6-96da-04f13d9b3f98",
   "metadata": {},
   "source": [
    "## 4.数据增强，主要参考了MindsSpore的[mindspore.dataset.vision](https://mindspore.cn/docs/zh-CN/r1.9/api_python/mindspore.dataset.vision.html) API\n",
    "\n",
    "其中变换方式添加了随机翻转，随机亮度对比度\n",
    "\n",
    "随机翻转从0.5提升到0.75的时候，验证集精度能提高1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e563c4-2004-4fe3-98b9-d061db91fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as vision\n",
    "\n",
    "def get_dataset():\n",
    "    image_folder_dataset_dir = \"./data/train\"\n",
    "    mapping = {}\n",
    "    for i in range(54):\n",
    "        mapping[str(i)] = int(i)\n",
    "\n",
    "    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "    transforms_list = [vision.RandomCropDecodeResize(size=224,\n",
    "                                                    scale=(0.08, 1.0),\n",
    "                                                    ratio=(0.75, 1.333)),\n",
    "                    vision.RandomHorizontalFlip(0.8),\n",
    "                    vision.RandomVerticalFlip(0.8),\n",
    "                    vision.RandomColorAdjust(brightness=(0.5, 1.25),contrast=(0.4, 1.1),saturation=(0.3, 1)),\n",
    "                    vision.Normalize(mean=mean, std=std),\n",
    "                    vision.HWC2CHW(),\n",
    "                    ]\n",
    "\n",
    "    dataset = ds.ImageFolderDataset(dataset_dir=image_folder_dataset_dir,\n",
    "                                    shuffle=True,\n",
    "                                    num_parallel_workers=8,\n",
    "                                    class_indexing=mapping\n",
    "                                    )\n",
    "\n",
    "    dataset = dataset.map(operations=transforms_list)\n",
    "\n",
    "    dataset = dataset.batch(32, drop_remainder=True, num_parallel_workers=8)\n",
    "\n",
    "    train_dataset, val_dataset = dataset.split([0.9, 0.1])\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e415f7-3660-483a-8129-273bf97953c2",
   "metadata": {},
   "source": [
    "建立网络，参考官方的ViT模型：\n",
    "[Vision Transformer图像分类](https://mindspore.cn/tutorials/application/zh-CN/r1.9/cv/vit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f03edd-7f15-4667-aa70-2b98b4e527ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import nn, ops\n",
    "import mindspore as ms\n",
    "from typing import Optional\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore import Parameter\n",
    "\n",
    "def init(init_type, shape, dtype, name, requires_grad):\n",
    "    \"\"\"Init.\"\"\"\n",
    "    initial = initializer(init_type, shape, dtype).init_data()\n",
    "    return Parameter(initial, name=name, requires_grad=requires_grad)\n",
    "\n",
    "class ViT(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 input_channels: int = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embed_dim: int = 768,\n",
    "                 num_layers: int = 12,\n",
    "                 num_heads: int = 12,\n",
    "                 mlp_dim: int = 3072,\n",
    "                 keep_prob: float = 1.0,\n",
    "                 attention_keep_prob: float = 1.0,\n",
    "                 drop_path_keep_prob: float = 1.0,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 norm: Optional[nn.Cell] = nn.LayerNorm,\n",
    "                 pool: str = 'cls',\n",
    "                 num_classes=1000) -> None:\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(image_size=image_size,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embed_dim=embed_dim,\n",
    "                                              input_channels=input_channels)\n",
    "        num_patches = self.patch_embedding.num_patches\n",
    "\n",
    "        # 此处增加class_embedding和pos_embedding，如果不是进行分类任务\n",
    "        # 可以只增加pos_embedding，通过pool参数进行控制\n",
    "        self.cls_token = init(init_type=Normal(sigma=1.0),\n",
    "                              shape=(1, 1, embed_dim),\n",
    "                              dtype=ms.float32,\n",
    "                              name='cls',\n",
    "                              requires_grad=True)\n",
    "\n",
    "        # pos_embedding也是一组可以学习的参数，会被加入到经过处理的patch矩阵中\n",
    "        self.pos_embedding = init(init_type=Normal(sigma=1.0),\n",
    "                                  shape=(1, num_patches + 1, embed_dim),\n",
    "                                  dtype=ms.float32,\n",
    "                                  name='pos_embedding',\n",
    "                                  requires_grad=True)\n",
    "\n",
    "        # axis=1定义了会在向量的开头加入class_embedding\n",
    "        self.concat = ops.Concat(axis=1)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.pos_dropout = nn.Dropout(keep_prob)\n",
    "        self.norm = norm((embed_dim,))\n",
    "        self.tile = ops.Tile()\n",
    "        self.transformer = TransformerEncoder(dim=embed_dim,\n",
    "                                              num_layers=num_layers,\n",
    "                                              num_heads=num_heads,\n",
    "                                              mlp_dim=mlp_dim,\n",
    "                                              keep_prob=keep_prob,\n",
    "                                              attention_keep_prob=attention_keep_prob,\n",
    "                                              drop_path_keep_prob=drop_path_keep_prob,\n",
    "                                              activation=activation,\n",
    "                                              norm=norm)\n",
    "        self.dropout = nn.Dropout(keep_prob)\n",
    "        self.dense = nn.Dense(embed_dim, num_classes)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ViT construct.\"\"\"\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # class_embedding主要借鉴了BERT模型的用于文本分类时的思想\n",
    "        # 在每一个word vector之前增加一个类别值，通常是加在向量的第一位\n",
    "        cls_tokens = self.tile(self.cls_token, (x.shape[0], 1, 1))\n",
    "        x = self.concat((cls_tokens, x))\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        x = self.pos_dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # 增加的class_embedding是一个可以学习的参数，经过网络的不断训练\n",
    "        # 最终以输出向量的第一个维度的输出来决定最后的输出类别；\n",
    "        x = x[:, 0]\n",
    "\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 keep_prob: float = 1.0,\n",
    "                 attention_keep_prob: float = 1.0):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = ms.Tensor(head_dim ** -0.5)\n",
    "\n",
    "        self.qkv = nn.Dense(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(attention_keep_prob)\n",
    "        self.out = nn.Dense(dim, dim)\n",
    "        self.out_drop = nn.Dropout(keep_prob)\n",
    "\n",
    "        self.mul = ops.Mul()\n",
    "        self.reshape = ops.Reshape()\n",
    "        self.transpose = ops.Transpose()\n",
    "        self.unstack = ops.Unstack(axis=0)\n",
    "        self.attn_matmul_v = ops.BatchMatMul()\n",
    "        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Attention construct.\"\"\"\n",
    "        b, n, c = x.shape\n",
    "\n",
    "        # 最初的输入向量首先会经过Embedding层映射成Q(Query)，K(Key)，V(Value)三个向量\n",
    "        # 由于是并行操作，所以代码中是映射成为dim*3的向量然后进行分割\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        #多头注意力机制就是将原本self-Attention处理的向量分割为多个Head进行处理\n",
    "        qkv = self.reshape(qkv, (b, n, 3, self.num_heads, c // self.num_heads))\n",
    "        qkv = self.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = self.unstack(qkv)\n",
    "\n",
    "        # 自注意力机制的自注意主要体现在它的Q，K，V都来源于其自身\n",
    "        # 也就是该过程是在提取输入的不同顺序的向量的联系与特征\n",
    "        # 最终通过不同顺序向量之间的联系紧密性（Q与K乘积经过Softmax的结果）来表现出来\n",
    "        attn = self.q_matmul_k(q, k)\n",
    "        attn = self.mul(attn, self.scale)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # 其最终输出则是通过V这个映射后的向量与QK经过Softmax结果进行weight sum获得\n",
    "        # 这个过程可以理解为在全局上进行自注意表示\n",
    "        out = self.attn_matmul_v(attn, v)\n",
    "        out = self.transpose(out, (0, 2, 1, 3))\n",
    "        out = self.reshape(out, (b, n, c))\n",
    "        out = self.out(out)\n",
    "        out = self.out_drop(out)\n",
    "        \n",
    "\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 hidden_features: Optional[int] = None,\n",
    "                 out_features: Optional[int] = None,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 keep_prob: float = 1.0):\n",
    "        super(FeedForward, self).__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.dense1 = nn.Dense(in_features, hidden_features)\n",
    "        self.activation = activation()\n",
    "        self.dense2 = nn.Dense(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(keep_prob)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Feed Forward construct.\"\"\"\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualCell(nn.Cell):\n",
    "    def __init__(self, cell):\n",
    "        super(ResidualCell, self).__init__()\n",
    "        self.cell = cell\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ResidualCell construct.\"\"\"\n",
    "        return self.cell(x) + x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 num_layers: int,\n",
    "                 num_heads: int,\n",
    "                 mlp_dim: int,\n",
    "                 keep_prob: float = 1.,\n",
    "                 attention_keep_prob: float = 1.0,\n",
    "                 drop_path_keep_prob: float = 1.0,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 norm: nn.Cell = nn.LayerNorm):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        # 从vit_architecture图可以发现，多个子encoder的堆叠就完成了模型编码器的构建\n",
    "        # 在ViT模型中，依然沿用这个思路，通过配置超参数num_layers，就可以确定堆叠层数\n",
    "        for _ in range(num_layers):\n",
    "            normalization1 = norm((dim,))\n",
    "            normalization2 = norm((dim,))\n",
    "            attention = Attention(dim=dim,\n",
    "                                  num_heads=num_heads,\n",
    "                                  keep_prob=keep_prob,\n",
    "                                  attention_keep_prob=attention_keep_prob)\n",
    "\n",
    "            feedforward = FeedForward(in_features=dim,\n",
    "                                      hidden_features=mlp_dim,\n",
    "                                      activation=activation,\n",
    "                                      keep_prob=keep_prob)\n",
    "\n",
    "            # ViT模型中的基础结构与标准Transformer有所不同\n",
    "            # 主要在于Normalization的位置是放在Self-Attention和Feed Forward之前\n",
    "            # 其他结构如Residual Connection，Feed Forward，Normalization都如Transformer中所设计\n",
    "            layers.append(\n",
    "                nn.SequentialCell([\n",
    "                    # Residual Connection，Normalization的结构可以保证模型有很强的扩展性\n",
    "                    # 保证信息经过深层处理不会出现退化的现象，这是Residual Connection的作用\n",
    "                    # Normalization和dropout的应用可以增强模型泛化能力\n",
    "                    ResidualCell(nn.SequentialCell([normalization1,\n",
    "                                                    attention])),\n",
    "\n",
    "                    ResidualCell(nn.SequentialCell([normalization2,\n",
    "                                                    feedforward]))\n",
    "                ])\n",
    "            )\n",
    "        self.layers = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Transformer construct.\"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Cell):\n",
    "    MIN_NUM_PATCHES = 4\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 patch_size: int = 16,\n",
    "                 embed_dim: int = 768,\n",
    "                 input_channels: int = 3):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # 通过将输入图像在每个channel上划分为16*16个patch\n",
    "        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=patch_size, stride=patch_size, has_bias=True)\n",
    "        self.reshape = ops.Reshape()\n",
    "        self.transpose = ops.Transpose()\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Path Embedding construct.\"\"\"\n",
    "        x = self.conv(x)\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # 再将每一个patch的矩阵拉伸成为一个1维向量，从而获得了近似词向量堆叠的效果；\n",
    "        x = self.reshape(x, (b, c, h * w))\n",
    "        x = self.transpose(x, (0, 2, 1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd108bc0-cb34-4602-bdf9-5714b4864b50",
   "metadata": {},
   "source": [
    "定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a09e01-31ec-4262-80af-3b34196b3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.nn import LossBase\n",
    "from mindspore import nn, ops\n",
    "import mindspore as ms\n",
    "from mindspore.common.initializer import One\n",
    "\n",
    "class CrossEntropySmooth(LossBase):\n",
    "    \"\"\"CrossEntropy.\"\"\"\n",
    "\n",
    "    def __init__(self, sparse=True, reduction='mean', smooth_factor=0., num_classes=1000):\n",
    "        super(CrossEntropySmooth, self).__init__()\n",
    "        self.onehot = ops.OneHot()\n",
    "        self.sparse = sparse\n",
    "        self.on_value = ms.Tensor(1.0 - smooth_factor, ms.float32)\n",
    "        self.off_value = ms.Tensor(1.0 * smooth_factor / (num_classes - 1), ms.float32)\n",
    "        self.ce = nn.SoftmaxCrossEntropyWithLogits(reduction=reduction)\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        if self.sparse:\n",
    "            label = self.onehot(label, ops.shape(logit)[1], self.on_value, self.off_value)\n",
    "        loss = self.ce(logit, label)\n",
    "        return loss\n",
    "\n",
    "logits = ms.Tensor(shape = (32, 54), dtype=ms.float32, init=One())\n",
    "label = ms.Tensor(shape = (32,), dtype=ms.int32, init=One())\n",
    "network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                  reduction=\"mean\",\n",
    "                                  smooth_factor=0.1,\n",
    "                                  num_classes=54)\n",
    "loss = network_loss(logits, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4a3a9-5d99-4f61-9d09-dcf165c89d57",
   "metadata": {},
   "source": [
    "定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e454ff8-3863-4f9a-990f-356f484f77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "# from vit import ViT\n",
    "from mindspore.train.callback import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n",
    "from mindspore import nn, context\n",
    "# from mindvision.engine.callback import ValAccMonitor\n",
    "\n",
    "\n",
    "class CustomWithLossCell(nn.Cell):\n",
    "    \"\"\"连接前向网络和损失函数\"\"\"\n",
    "\n",
    "    def __init__(self, backbone, loss_fn):\n",
    "        \"\"\"前向网络backbone和损失函数loss_fn\"\"\"\n",
    "        super(CustomWithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._backbone = backbone\n",
    "        self._loss_fn = loss_fn\n",
    "\n",
    "    def construct(self, data, label):\n",
    "        output = self._backbone(data)                 # 前向计算得到网络输出\n",
    "        return self._loss_fn(output, label)  # 得到多标签损失值\n",
    "\n",
    "class CustomWithEvalCell(nn.Cell):\n",
    "    \"\"\"自定义多标签评估网络\"\"\"\n",
    "\n",
    "    def __init__(self, network):\n",
    "        super(CustomWithEvalCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "\n",
    "    def construct(self, data, label):\n",
    "        output = self.network(data)\n",
    "        return output, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ebb6d7-3e32-4af2-808f-c12c876d128a",
   "metadata": {},
   "source": [
    "开示训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "691d57e9-adaa-4289-b5a1-0fb4d7cd89da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(6018:281473032386432,MainProcess):2023-01-15-07:43:09.298.362 [mindspore/dataset/engine/datasets.py:1122] Dataset is shuffled before split.\n",
      "[WARNING] DEVICE(6018,ffff8c1bf780,python):2023-01-15-07:43:38.895.369 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 104, loss is 2.5859375\n",
      "epoch time: 74268.370 ms, per step time: 714.119 ms\n",
      "epoch: 2 step: 104, loss is 1.4765625\n",
      "epoch time: 10257.620 ms, per step time: 98.631 ms\n",
      "epoch: 3 step: 104, loss is 1.1494140625\n",
      "epoch time: 14763.643 ms, per step time: 141.958 ms\n",
      "epoch: 4 step: 104, loss is 1.0556640625\n",
      "epoch time: 10262.417 ms, per step time: 98.677 ms\n",
      "epoch: 5 step: 104, loss is 0.94287109375\n",
      "epoch time: 10266.959 ms, per step time: 98.721 ms\n",
      "epoch: 6 step: 104, loss is 0.974609375\n",
      "epoch time: 14379.265 ms, per step time: 138.262 ms\n",
      "epoch: 7 step: 104, loss is 0.7646484375\n",
      "epoch time: 10269.441 ms, per step time: 98.745 ms\n",
      "epoch: 8 step: 104, loss is 0.8125\n",
      "epoch time: 10269.266 ms, per step time: 98.743 ms\n",
      "epoch: 9 step: 104, loss is 0.81787109375\n",
      "epoch time: 14172.830 ms, per step time: 136.277 ms\n",
      "epoch: 10 step: 104, loss is 0.79833984375\n",
      "epoch time: 10257.586 ms, per step time: 98.631 ms\n",
      "epoch: 11 step: 104, loss is 0.78466796875\n",
      "epoch time: 10252.382 ms, per step time: 98.581 ms\n",
      "epoch: 12 step: 104, loss is 0.76025390625\n",
      "epoch time: 14431.830 ms, per step time: 138.768 ms\n",
      "epoch: 13 step: 104, loss is 0.85546875\n",
      "epoch time: 10263.094 ms, per step time: 98.684 ms\n",
      "epoch: 14 step: 104, loss is 0.8623046875\n",
      "epoch time: 10256.391 ms, per step time: 98.619 ms\n",
      "epoch: 15 step: 104, loss is 0.76171875\n",
      "epoch time: 14989.735 ms, per step time: 144.132 ms\n",
      "epoch: 16 step: 104, loss is 0.8623046875\n",
      "epoch time: 10258.482 ms, per step time: 98.639 ms\n",
      "epoch: 17 step: 104, loss is 0.87158203125\n",
      "epoch time: 10251.768 ms, per step time: 98.575 ms\n",
      "epoch: 18 step: 104, loss is 0.8359375\n",
      "epoch time: 14206.444 ms, per step time: 136.600 ms\n",
      "epoch: 19 step: 104, loss is 0.7587890625\n",
      "epoch time: 10252.415 ms, per step time: 98.581 ms\n",
      "epoch: 20 step: 104, loss is 0.86865234375\n",
      "epoch time: 10245.607 ms, per step time: 98.515 ms\n",
      "epoch: 21 step: 104, loss is 0.8095703125\n",
      "epoch time: 14349.715 ms, per step time: 137.978 ms\n",
      "epoch: 22 step: 104, loss is 0.8486328125\n",
      "epoch time: 10285.455 ms, per step time: 98.899 ms\n",
      "epoch: 23 step: 104, loss is 1.056640625\n",
      "epoch time: 10250.073 ms, per step time: 98.558 ms\n",
      "epoch: 24 step: 104, loss is 0.77490234375\n",
      "epoch time: 15529.114 ms, per step time: 149.318 ms\n",
      "epoch: 25 step: 104, loss is 0.94873046875\n",
      "epoch time: 10246.897 ms, per step time: 98.528 ms\n",
      "epoch: 26 step: 104, loss is 0.77099609375\n",
      "epoch time: 10328.032 ms, per step time: 99.308 ms\n",
      "epoch: 27 step: 104, loss is 0.9462890625\n",
      "epoch time: 14491.949 ms, per step time: 139.346 ms\n",
      "epoch: 28 step: 104, loss is 0.771484375\n",
      "epoch time: 10248.447 ms, per step time: 98.543 ms\n",
      "epoch: 29 step: 104, loss is 0.880859375\n",
      "epoch time: 10244.425 ms, per step time: 98.504 ms\n",
      "epoch: 30 step: 104, loss is 0.7958984375\n",
      "epoch time: 15777.902 ms, per step time: 151.711 ms\n",
      "epoch: 31 step: 104, loss is 0.76416015625\n",
      "epoch time: 10251.228 ms, per step time: 98.569 ms\n",
      "epoch: 32 step: 104, loss is 0.94677734375\n",
      "epoch time: 10243.269 ms, per step time: 98.493 ms\n",
      "epoch: 33 step: 104, loss is 0.75634765625\n",
      "epoch time: 14289.706 ms, per step time: 137.401 ms\n",
      "epoch: 34 step: 104, loss is 0.89794921875\n",
      "epoch time: 10245.707 ms, per step time: 98.516 ms\n",
      "epoch: 35 step: 104, loss is 0.9111328125\n",
      "epoch time: 10258.181 ms, per step time: 98.636 ms\n",
      "epoch: 36 step: 104, loss is 0.81005859375\n",
      "epoch time: 15520.983 ms, per step time: 149.240 ms\n",
      "epoch: 37 step: 104, loss is 0.86669921875\n",
      "epoch time: 10241.750 ms, per step time: 98.478 ms\n",
      "epoch: 38 step: 104, loss is 0.78759765625\n",
      "epoch time: 10242.641 ms, per step time: 98.487 ms\n",
      "epoch: 39 step: 104, loss is 0.82470703125\n",
      "epoch time: 14332.159 ms, per step time: 137.809 ms\n",
      "epoch: 40 step: 104, loss is 0.75537109375\n",
      "epoch time: 10253.596 ms, per step time: 98.592 ms\n",
      "{'acc': 0.9947916666666666}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ms.set_context(mode=ms.GRAPH_MODE,device_target=\"Ascend\")\n",
    "train_data, val_data = get_dataset()\n",
    "\n",
    "network = ViT()\n",
    "vit_path = \"./ckpt/vit_b_16_224.ckpt\"\n",
    "param_dict = ms.load_checkpoint(vit_path)\n",
    "ms.load_param_into_net(network, param_dict)\n",
    "network.dense = nn.Dense(768, 54)\n",
    "\n",
    "epoch_size = 40\n",
    "momentum = 0.9\n",
    "num_classes = 54\n",
    "step_size = train_data.get_dataset_size()\n",
    "\n",
    "lr = nn.cosine_decay_lr(min_lr=float(0),\n",
    "                        max_lr=0.00005,\n",
    "                        total_step=epoch_size * step_size,\n",
    "                        step_per_epoch=step_size,\n",
    "                        decay_epoch=10)\n",
    "network_opt = nn.Adam(network.trainable_params(), lr, momentum)\n",
    "network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                    reduction=\"mean\",\n",
    "                                    smooth_factor=0.1,\n",
    "                                    num_classes=num_classes)\n",
    "\n",
    "loss_net = CustomWithLossCell(network, network_loss)\n",
    "eval_net = CustomWithEvalCell(network)\n",
    "    \n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=3 * step_size, keep_checkpoint_max=100)\n",
    "ckpt_callback = ModelCheckpoint(prefix='vit_b_16', directory='./ViT1', config=ckpt_config)\n",
    "ascend_target = (ms.get_context(\"device_target\") == \"Ascend\")\n",
    "if ascend_target:\n",
    "    model = ms.Model(loss_net, optimizer=network_opt, eval_network=eval_net, metrics={\"acc\"}, amp_level=\"O2\")\n",
    "else:\n",
    "    model = ms.Model(loss_net, optimizer=network_opt, eval_network=eval_net, metrics={\"acc\"}, amp_level=\"O0\")\n",
    "model.train(epoch_size,\n",
    "            train_data,\n",
    "            callbacks=[ckpt_callback, LossMonitor(step_size), TimeMonitor(step_size)]\n",
    "            )\n",
    "result = model.eval(val_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5915eee-07cd-4a80-9707-ae2991c11971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] MD(6018,ffff8c1bf780,python):2023-01-15-07:54:32.340.027 [mindspore/ccsrc/minddata/dataset/engine/datasetops/device_queue_op.cc:73] ~DeviceQueueOp] preprocess_batch: 12; batch_queue: 7, 7, 6, 6, 5, 5, 4, 4, 3, 3; push_start_time: 2023-01-15-07:52:16.988.667, 2023-01-15-07:52:17.007.056, 2023-01-15-07:52:17.041.176, 2023-01-15-07:52:17.057.230, 2023-01-15-07:52:17.061.301, 2023-01-15-07:52:17.066.119, 2023-01-15-07:52:17.070.183, 2023-01-15-07:52:17.079.712, 2023-01-15-07:52:17.098.703, 2023-01-15-07:52:17.129.798; push_end_time: 2023-01-15-07:52:16.993.428, 2023-01-15-07:52:17.011.065, 2023-01-15-07:52:17.045.322, 2023-01-15-07:52:17.061.259, 2023-01-15-07:52:17.066.076, 2023-01-15-07:52:17.070.136, 2023-01-15-07:52:17.074.622, 2023-01-15-07:52:17.083.757, 2023-01-15-07:52:17.102.834, 2023-01-15-07:52:17.134.283.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.9869791666666666}\n"
     ]
    }
   ],
   "source": [
    "result = model.eval(val_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac8156-4823-455a-a900-94e29ff80b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "\n",
    "# from vit import ViT\n",
    "\n",
    "def process_image(path):\n",
    "\n",
    "    image_list = []\n",
    "\n",
    "    mean = np.array([0.485 * 255, 0.456 * 255, 0.406 * 255])\n",
    "    std = np.array([0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    files = sorted(files, key=lambda x:int(x.split('.')[0]))\n",
    "    \n",
    "    for file in files:\n",
    "        image = Image.open(path + file).convert(\"RGB\")\n",
    "        image = image.resize((224, 224))\n",
    "        # plt.imshow(image)\n",
    "        image = (image - mean) / std\n",
    "        image = image.astype(np.float32)\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image_list.append(image)\n",
    "    return image_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ms.set_context(device_target=\"CPU\")\n",
    "# ms.set_context(device_target=\"Ascend\")\n",
    "images = process_image(\"./test/\")\n",
    "\n",
    "network = ViT(num_classes=54)\n",
    "vit_path = \"./ViT1/vit_b_16-40_104.ckpt\"\n",
    "param_dict = ms.load_checkpoint(vit_path)\n",
    "ms.load_param_into_net(network, param_dict)\n",
    "\n",
    "model = ms.Model(network)\n",
    "\n",
    "results = []\n",
    "for image in images:\n",
    "    pre = model.predict(Tensor(image,ms.float32))\n",
    "    result = np.argmax(pre)\n",
    "    results.append(result)\n",
    "    print(result)\n",
    "        \n",
    "with open(\"result0114.txt\",\"w\") as f:\n",
    "    for result in results:\n",
    "        f.write(str(result))\n",
    "        f.write('\\n')\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15129f10-d22f-4c3f-b1e0-5f5357f8c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy('./ViT1/vit_b_16-40_104.ckpt','obs://mindcon00001/xian/vit_b_16-40_104.ckpt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
