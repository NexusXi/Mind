{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252ccab2-cfdd-4dca-bad1-4f54b8a55db0",
   "metadata": {},
   "source": [
    "# Mindcon 食物分类和西安旅游分类通用notebook\n",
    "\n",
    "# 1.下载数据到notebook环境\n",
    "\n",
    "以下是美食分类的数据\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4161b41-c59e-4293-9cc1-96fd58668f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy('obs://mindcon00001/raw/mindcon_food_classification.zip','/home/ma-user/work/mindcon_food_classification.zip')\n",
    "!unzip mindcon_food_classification.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7af44-0d48-4190-a319-3045fd2dcc49",
   "metadata": {},
   "source": [
    "## 2.打开终端，下载预训练模型\n",
    "```\n",
    "mkdir ckpt\n",
    "cd ckpt\n",
    "wget https://download.mindspore.cn/vision/classification/vit_b_16_224.ckpt\n",
    "pwd\n",
    "```\n",
    "## 3.处理数据方便MindSpore读取。\n",
    "\n",
    "处理成标签为文件夹，每个文件夹内含有对应图片，美食图片分类可以直接对照标签修改文件夹名\n",
    "\n",
    "## 4.数据增强，主要参考了MindsSpore的[mindspore.dataset.vision](https://mindspore.cn/docs/zh-CN/r1.9/api_python/mindspore.dataset.vision.html) API\n",
    "\n",
    "其中变换方式添加了随机翻转，随机亮度对比度\n",
    "\n",
    "随机翻转从0.5提升到0.75的时候，验证集精度能提高1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e563c4-2004-4fe3-98b9-d061db91fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mindspore.dataset as ds\n",
    "# import mindspore.dataset.vision.c_transforms as vision\n",
    "import mindspore.dataset.vision as vision\n",
    "\n",
    "def get_dataset():\n",
    "    image_folder_dataset_dir = \"./train\"\n",
    "    mapping = {}\n",
    "    for i in range(10):\n",
    "        mapping[str(i)] = int(i)\n",
    "\n",
    "    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "    transforms_list = [vision.RandomCropDecodeResize(size=224,\n",
    "                                                    scale=(0.08, 1.0),\n",
    "                                                    ratio=(0.75, 1.333)),\n",
    "                    vision.RandomHorizontalFlip(0.75),\n",
    "                    vision.RandomVerticalFlip(0.75),\n",
    "                    vision.Normalize(mean=mean, std=std),\n",
    "                    vision.HWC2CHW(),\n",
    "                    ]\n",
    "\n",
    "    dataset = ds.ImageFolderDataset(dataset_dir=image_folder_dataset_dir,\n",
    "                                    shuffle=True,\n",
    "                                    num_parallel_workers=8,\n",
    "                                    class_indexing=mapping\n",
    "                                    )\n",
    "\n",
    "    dataset = dataset.map(operations=transforms_list)\n",
    "\n",
    "    dataset = dataset.batch(32, drop_remainder=True, num_parallel_workers=8)\n",
    "\n",
    "    train_dataset, val_dataset = dataset.split([0.9, 0.1])\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736cadf-ab5f-433c-b62a-e9b9a45be41a",
   "metadata": {},
   "source": [
    "建立网络，参考官方的ViT模型：\n",
    "[Vision Transformer图像分类](https://mindspore.cn/tutorials/application/zh-CN/r1.9/cv/vit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1f03edd-7f15-4667-aa70-2b98b4e527ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import nn, ops\n",
    "import mindspore as ms\n",
    "from typing import Optional\n",
    "from mindspore.common.initializer import Normal\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore import Parameter\n",
    "\n",
    "def init(init_type, shape, dtype, name, requires_grad):\n",
    "    \"\"\"Init.\"\"\"\n",
    "    initial = initializer(init_type, shape, dtype).init_data()\n",
    "    return Parameter(initial, name=name, requires_grad=requires_grad)\n",
    "\n",
    "class ViT(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 input_channels: int = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embed_dim: int = 768,\n",
    "                 num_layers: int = 12,\n",
    "                 num_heads: int = 12,\n",
    "                 mlp_dim: int = 3072,\n",
    "                 keep_prob: float = 1.0,\n",
    "                 attention_keep_prob: float = 1.0,\n",
    "                 drop_path_keep_prob: float = 1.0,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 norm: Optional[nn.Cell] = nn.LayerNorm,\n",
    "                 pool: str = 'cls',\n",
    "                 num_classes=1000) -> None:\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(image_size=image_size,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embed_dim=embed_dim,\n",
    "                                              input_channels=input_channels)\n",
    "        num_patches = self.patch_embedding.num_patches\n",
    "\n",
    "        # 此处增加class_embedding和pos_embedding，如果不是进行分类任务\n",
    "        # 可以只增加pos_embedding，通过pool参数进行控制\n",
    "        self.cls_token = init(init_type=Normal(sigma=1.0),\n",
    "                              shape=(1, 1, embed_dim),\n",
    "                              dtype=ms.float32,\n",
    "                              name='cls',\n",
    "                              requires_grad=True)\n",
    "\n",
    "        # pos_embedding也是一组可以学习的参数，会被加入到经过处理的patch矩阵中\n",
    "        self.pos_embedding = init(init_type=Normal(sigma=1.0),\n",
    "                                  shape=(1, num_patches + 1, embed_dim),\n",
    "                                  dtype=ms.float32,\n",
    "                                  name='pos_embedding',\n",
    "                                  requires_grad=True)\n",
    "\n",
    "        # axis=1定义了会在向量的开头加入class_embedding\n",
    "        self.concat = ops.Concat(axis=1)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.pos_dropout = nn.Dropout(keep_prob)\n",
    "        self.norm = norm((embed_dim,))\n",
    "        self.tile = ops.Tile()\n",
    "        self.transformer = TransformerEncoder(dim=embed_dim,\n",
    "                                              num_layers=num_layers,\n",
    "                                              num_heads=num_heads,\n",
    "                                              mlp_dim=mlp_dim,\n",
    "                                              keep_prob=keep_prob,\n",
    "                                              attention_keep_prob=attention_keep_prob,\n",
    "                                              drop_path_keep_prob=drop_path_keep_prob,\n",
    "                                              activation=activation,\n",
    "                                              norm=norm)\n",
    "        self.dropout = nn.Dropout(keep_prob)\n",
    "        self.dense = nn.Dense(embed_dim, num_classes)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ViT construct.\"\"\"\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # class_embedding主要借鉴了BERT模型的用于文本分类时的思想\n",
    "        # 在每一个word vector之前增加一个类别值，通常是加在向量的第一位\n",
    "        cls_tokens = self.tile(self.cls_token, (x.shape[0], 1, 1))\n",
    "        x = self.concat((cls_tokens, x))\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        x = self.pos_dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # 增加的class_embedding是一个可以学习的参数，经过网络的不断训练\n",
    "        # 最终以输出向量的第一个维度的输出来决定最后的输出类别；\n",
    "        x = x[:, 0]\n",
    "\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 keep_prob: float = 1.0,\n",
    "                 attention_keep_prob: float = 1.0):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = ms.Tensor(head_dim ** -0.5)\n",
    "\n",
    "        self.qkv = nn.Dense(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(attention_keep_prob)\n",
    "        self.out = nn.Dense(dim, dim)\n",
    "        self.out_drop = nn.Dropout(keep_prob)\n",
    "\n",
    "        self.mul = ops.Mul()\n",
    "        self.reshape = ops.Reshape()\n",
    "        self.transpose = ops.Transpose()\n",
    "        self.unstack = ops.Unstack(axis=0)\n",
    "        self.attn_matmul_v = ops.BatchMatMul()\n",
    "        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Attention construct.\"\"\"\n",
    "        b, n, c = x.shape\n",
    "\n",
    "        # 最初的输入向量首先会经过Embedding层映射成Q(Query)，K(Key)，V(Value)三个向量\n",
    "        # 由于是并行操作，所以代码中是映射成为dim*3的向量然后进行分割\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        #多头注意力机制就是将原本self-Attention处理的向量分割为多个Head进行处理\n",
    "        qkv = self.reshape(qkv, (b, n, 3, self.num_heads, c // self.num_heads))\n",
    "        qkv = self.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = self.unstack(qkv)\n",
    "\n",
    "        # 自注意力机制的自注意主要体现在它的Q，K，V都来源于其自身\n",
    "        # 也就是该过程是在提取输入的不同顺序的向量的联系与特征\n",
    "        # 最终通过不同顺序向量之间的联系紧密性（Q与K乘积经过Softmax的结果）来表现出来\n",
    "        attn = self.q_matmul_k(q, k)\n",
    "        attn = self.mul(attn, self.scale)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # 其最终输出则是通过V这个映射后的向量与QK经过Softmax结果进行weight sum获得\n",
    "        # 这个过程可以理解为在全局上进行自注意表示\n",
    "        out = self.attn_matmul_v(attn, v)\n",
    "        out = self.transpose(out, (0, 2, 1, 3))\n",
    "        out = self.reshape(out, (b, n, c))\n",
    "        out = self.out(out)\n",
    "        out = self.out_drop(out)\n",
    "        \n",
    "\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 hidden_features: Optional[int] = None,\n",
    "                 out_features: Optional[int] = None,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 keep_prob: float = 1.0):\n",
    "        super(FeedForward, self).__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.dense1 = nn.Dense(in_features, hidden_features)\n",
    "        self.activation = activation()\n",
    "        self.dense2 = nn.Dense(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(keep_prob)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Feed Forward construct.\"\"\"\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualCell(nn.Cell):\n",
    "    def __init__(self, cell):\n",
    "        super(ResidualCell, self).__init__()\n",
    "        self.cell = cell\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ResidualCell construct.\"\"\"\n",
    "        return self.cell(x) + x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 num_layers: int,\n",
    "                 num_heads: int,\n",
    "                 mlp_dim: int,\n",
    "                 keep_prob: float = 1.,\n",
    "                 attention_keep_prob: float = 1.0,\n",
    "                 drop_path_keep_prob: float = 1.0,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 norm: nn.Cell = nn.LayerNorm):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        # 从vit_architecture图可以发现，多个子encoder的堆叠就完成了模型编码器的构建\n",
    "        # 在ViT模型中，依然沿用这个思路，通过配置超参数num_layers，就可以确定堆叠层数\n",
    "        for _ in range(num_layers):\n",
    "            normalization1 = norm((dim,))\n",
    "            normalization2 = norm((dim,))\n",
    "            attention = Attention(dim=dim,\n",
    "                                  num_heads=num_heads,\n",
    "                                  keep_prob=keep_prob,\n",
    "                                  attention_keep_prob=attention_keep_prob)\n",
    "\n",
    "            feedforward = FeedForward(in_features=dim,\n",
    "                                      hidden_features=mlp_dim,\n",
    "                                      activation=activation,\n",
    "                                      keep_prob=keep_prob)\n",
    "\n",
    "            # ViT模型中的基础结构与标准Transformer有所不同\n",
    "            # 主要在于Normalization的位置是放在Self-Attention和Feed Forward之前\n",
    "            # 其他结构如Residual Connection，Feed Forward，Normalization都如Transformer中所设计\n",
    "            layers.append(\n",
    "                nn.SequentialCell([\n",
    "                    # Residual Connection，Normalization的结构可以保证模型有很强的扩展性\n",
    "                    # 保证信息经过深层处理不会出现退化的现象，这是Residual Connection的作用\n",
    "                    # Normalization和dropout的应用可以增强模型泛化能力\n",
    "                    ResidualCell(nn.SequentialCell([normalization1,\n",
    "                                                    attention])),\n",
    "\n",
    "                    ResidualCell(nn.SequentialCell([normalization2,\n",
    "                                                    feedforward]))\n",
    "                ])\n",
    "            )\n",
    "        self.layers = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Transformer construct.\"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Cell):\n",
    "    MIN_NUM_PATCHES = 4\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 patch_size: int = 16,\n",
    "                 embed_dim: int = 768,\n",
    "                 input_channels: int = 3):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # 通过将输入图像在每个channel上划分为16*16个patch\n",
    "        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=patch_size, stride=patch_size, has_bias=True)\n",
    "        self.reshape = ops.Reshape()\n",
    "        self.transpose = ops.Transpose()\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Path Embedding construct.\"\"\"\n",
    "        x = self.conv(x)\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # 再将每一个patch的矩阵拉伸成为一个1维向量，从而获得了近似词向量堆叠的效果；\n",
    "        x = self.reshape(x, (b, c, h * w))\n",
    "        x = self.transpose(x, (0, 2, 1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbeb1c-abd2-425a-861e-09163a6179ec",
   "metadata": {},
   "source": [
    "定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4a09e01-31ec-4262-80af-3b34196b3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.nn import LossBase\n",
    "from mindspore import nn, ops\n",
    "import mindspore as ms\n",
    "from mindspore.common.initializer import One\n",
    "\n",
    "class CrossEntropySmooth(LossBase):\n",
    "    \"\"\"CrossEntropy.\"\"\"\n",
    "\n",
    "    def __init__(self, sparse=True, reduction='mean', smooth_factor=0., num_classes=1000):\n",
    "        super(CrossEntropySmooth, self).__init__()\n",
    "        self.onehot = ops.OneHot()\n",
    "        self.sparse = sparse\n",
    "        self.on_value = ms.Tensor(1.0 - smooth_factor, ms.float32)\n",
    "        self.off_value = ms.Tensor(1.0 * smooth_factor / (num_classes - 1), ms.float32)\n",
    "        self.ce = nn.SoftmaxCrossEntropyWithLogits(reduction=reduction)\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        if self.sparse:\n",
    "            label = self.onehot(label, ops.shape(logit)[1], self.on_value, self.off_value)\n",
    "        loss = self.ce(logit, label)\n",
    "        return loss\n",
    "\n",
    "logits = ms.Tensor(shape = (32, 10), dtype=ms.float32, init=One())\n",
    "label = ms.Tensor(shape = (32,), dtype=ms.int32, init=One())\n",
    "network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                  reduction=\"mean\",\n",
    "                                  smooth_factor=0.1,\n",
    "                                  num_classes=10)\n",
    "loss = network_loss(logits, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f65a33-f114-4517-ab67-67183a553453",
   "metadata": {},
   "source": [
    "定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97f57c-7e1f-476e-8431-f2e391dc6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "# from vit import ViT\n",
    "from mindspore.train.callback import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n",
    "from mindspore import nn, context\n",
    "# from mindvision.engine.callback import ValAccMonitor\n",
    "\n",
    "\n",
    "class CustomWithLossCell(nn.Cell):\n",
    "    \"\"\"连接前向网络和损失函数\"\"\"\n",
    "\n",
    "    def __init__(self, backbone, loss_fn):\n",
    "        \"\"\"前向网络backbone和损失函数loss_fn\"\"\"\n",
    "        super(CustomWithLossCell, self).__init__(auto_prefix=False)\n",
    "        self._backbone = backbone\n",
    "        self._loss_fn = loss_fn\n",
    "\n",
    "    def construct(self, data, label):\n",
    "        output = self._backbone(data)                 # 前向计算得到网络输出\n",
    "        return self._loss_fn(output, label)  # 得到多标签损失值\n",
    "\n",
    "class CustomWithEvalCell(nn.Cell):\n",
    "    \"\"\"自定义多标签评估网络\"\"\"\n",
    "\n",
    "    def __init__(self, network):\n",
    "        super(CustomWithEvalCell, self).__init__(auto_prefix=False)\n",
    "        self.network = network\n",
    "\n",
    "    def construct(self, data, label):\n",
    "        output = self.network(data)\n",
    "        return output, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911f9ee-72d1-4a6e-81ef-52aad43de365",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "691d57e9-adaa-4289-b5a1-0fb4d7cd89da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(32385:281473638029184,MainProcess):2023-01-15-07:29:48.788.730 [mindspore/dataset/engine/datasets.py:1122] Dataset is shuffled before split.\n",
      "[WARNING] DEVICE(32385,ffffb0355780,python):2023-01-15-07:30:16.626.461 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[StridedSliceGrad] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 126, loss is 0.7470703125\n",
      "epoch time: 80074.762 ms, per step time: 635.514 ms\n",
      "epoch: 2 step: 126, loss is 0.63134765625\n",
      "epoch time: 13280.406 ms, per step time: 105.400 ms\n",
      "epoch: 3 step: 126, loss is 0.69091796875\n",
      "epoch time: 17897.302 ms, per step time: 142.042 ms\n",
      "epoch: 4 step: 126, loss is 0.5615234375\n",
      "epoch time: 13280.550 ms, per step time: 105.401 ms\n",
      "epoch: 5 step: 126, loss is 0.5830078125\n",
      "epoch time: 13275.676 ms, per step time: 105.363 ms\n",
      "epoch: 6 step: 126, loss is 0.5947265625\n",
      "epoch time: 17813.303 ms, per step time: 141.375 ms\n",
      "epoch: 7 step: 126, loss is 0.5986328125\n",
      "epoch time: 13291.845 ms, per step time: 105.491 ms\n",
      "epoch: 8 step: 126, loss is 0.58349609375\n",
      "epoch time: 13284.360 ms, per step time: 105.431 ms\n",
      "epoch: 9 step: 126, loss is 0.5888671875\n",
      "epoch time: 17414.108 ms, per step time: 138.207 ms\n",
      "epoch: 10 step: 126, loss is 0.55615234375\n",
      "epoch time: 13276.757 ms, per step time: 105.371 ms\n",
      "epoch: 11 step: 126, loss is 0.5625\n",
      "epoch time: 13272.435 ms, per step time: 105.337 ms\n",
      "epoch: 12 step: 126, loss is 0.55126953125\n",
      "epoch time: 17404.835 ms, per step time: 138.134 ms\n",
      "epoch: 13 step: 126, loss is 0.5556640625\n",
      "epoch time: 13275.590 ms, per step time: 105.362 ms\n",
      "epoch: 14 step: 126, loss is 0.55615234375\n",
      "epoch time: 13272.692 ms, per step time: 105.339 ms\n",
      "epoch: 15 step: 126, loss is 0.55029296875\n",
      "epoch time: 17449.641 ms, per step time: 138.489 ms\n",
      "epoch: 16 step: 126, loss is 0.5478515625\n",
      "epoch time: 13301.127 ms, per step time: 105.564 ms\n",
      "epoch: 17 step: 126, loss is 0.55078125\n",
      "epoch time: 13290.783 ms, per step time: 105.482 ms\n",
      "epoch: 18 step: 126, loss is 0.59228515625\n",
      "epoch time: 17764.941 ms, per step time: 140.992 ms\n",
      "epoch: 19 step: 126, loss is 0.5556640625\n",
      "epoch time: 13285.831 ms, per step time: 105.443 ms\n",
      "epoch: 20 step: 126, loss is 0.55078125\n",
      "epoch time: 13294.341 ms, per step time: 105.511 ms\n",
      "epoch: 21 step: 126, loss is 0.56201171875\n",
      "epoch time: 17545.926 ms, per step time: 139.253 ms\n",
      "epoch: 22 step: 126, loss is 0.55126953125\n",
      "epoch time: 13271.213 ms, per step time: 105.327 ms\n",
      "epoch: 23 step: 126, loss is 0.5869140625\n",
      "epoch time: 13270.155 ms, per step time: 105.319 ms\n",
      "epoch: 24 step: 126, loss is 0.5546875\n",
      "epoch time: 17023.141 ms, per step time: 135.104 ms\n",
      "epoch: 25 step: 126, loss is 0.55859375\n",
      "epoch time: 13274.581 ms, per step time: 105.354 ms\n",
      "epoch: 26 step: 126, loss is 0.5478515625\n",
      "epoch time: 13273.633 ms, per step time: 105.346 ms\n",
      "epoch: 27 step: 126, loss is 0.552734375\n",
      "epoch time: 17318.598 ms, per step time: 137.449 ms\n",
      "epoch: 28 step: 126, loss is 0.5888671875\n",
      "epoch time: 13274.365 ms, per step time: 105.352 ms\n",
      "epoch: 29 step: 126, loss is 0.55126953125\n",
      "epoch time: 13273.060 ms, per step time: 105.342 ms\n",
      "epoch: 30 step: 126, loss is 0.55078125\n",
      "epoch time: 17417.792 ms, per step time: 138.236 ms\n",
      "epoch: 31 step: 126, loss is 0.55712890625\n",
      "epoch time: 13276.078 ms, per step time: 105.366 ms\n",
      "epoch: 32 step: 126, loss is 0.58154296875\n",
      "epoch time: 13272.422 ms, per step time: 105.337 ms\n",
      "epoch: 33 step: 126, loss is 0.6875\n",
      "epoch time: 18641.084 ms, per step time: 147.945 ms\n",
      "epoch: 34 step: 126, loss is 0.55029296875\n",
      "epoch time: 13276.785 ms, per step time: 105.371 ms\n",
      "epoch: 35 step: 126, loss is 0.5498046875\n",
      "epoch time: 13295.116 ms, per step time: 105.517 ms\n",
      "epoch: 36 step: 126, loss is 0.6982421875\n",
      "epoch time: 17465.267 ms, per step time: 138.613 ms\n",
      "epoch: 37 step: 126, loss is 0.56103515625\n",
      "epoch time: 13271.146 ms, per step time: 105.327 ms\n",
      "epoch: 38 step: 126, loss is 0.60888671875\n",
      "epoch time: 13275.031 ms, per step time: 105.357 ms\n",
      "epoch: 39 step: 126, loss is 0.5673828125\n",
      "epoch time: 18067.736 ms, per step time: 143.395 ms\n",
      "epoch: 40 step: 126, loss is 0.54833984375\n",
      "epoch time: 13273.342 ms, per step time: 105.344 ms\n",
      "epoch: 41 step: 126, loss is 0.55029296875\n",
      "epoch time: 13288.263 ms, per step time: 105.462 ms\n",
      "epoch: 42 step: 126, loss is 0.583984375\n",
      "epoch time: 17505.296 ms, per step time: 138.931 ms\n",
      "epoch: 43 step: 126, loss is 0.58544921875\n",
      "epoch time: 13338.793 ms, per step time: 105.863 ms\n",
      "epoch: 44 step: 126, loss is 0.5654296875\n",
      "epoch time: 13323.132 ms, per step time: 105.739 ms\n",
      "epoch: 45 step: 126, loss is 0.796875\n",
      "epoch time: 19022.221 ms, per step time: 150.970 ms\n",
      "epoch: 46 step: 126, loss is 0.5498046875\n",
      "epoch time: 13276.495 ms, per step time: 105.369 ms\n",
      "epoch: 47 step: 126, loss is 0.56591796875\n",
      "epoch time: 13293.958 ms, per step time: 105.508 ms\n",
      "epoch: 48 step: 126, loss is 0.5517578125\n",
      "epoch time: 17594.214 ms, per step time: 139.637 ms\n",
      "epoch: 49 step: 126, loss is 0.5791015625\n",
      "epoch time: 13283.134 ms, per step time: 105.422 ms\n",
      "epoch: 50 step: 126, loss is 0.5576171875\n",
      "epoch time: 13278.661 ms, per step time: 105.386 ms\n",
      "{'acc': 0.9776785714285714}\n"
     ]
    }
   ],
   "source": [
    "ms.set_context(mode=ms.GRAPH_MODE,device_target=\"Ascend\")\n",
    "train_data, val_data = get_dataset()\n",
    "\n",
    "network = ViT()\n",
    "vit_path = \"./ckpt/vit_b_16_224.ckpt\"\n",
    "param_dict = ms.load_checkpoint(vit_path)\n",
    "ms.load_param_into_net(network, param_dict)\n",
    "network.dense = nn.Dense(768, 10)\n",
    "\n",
    "epoch_size = 50\n",
    "momentum = 0.9\n",
    "num_classes = 10\n",
    "step_size = train_data.get_dataset_size()\n",
    "\n",
    "lr = nn.cosine_decay_lr(min_lr=float(0),\n",
    "                        max_lr=0.00005,\n",
    "                        total_step=epoch_size * step_size,\n",
    "                        step_per_epoch=step_size,\n",
    "                        decay_epoch=10)\n",
    "network_opt = nn.Adam(network.trainable_params(), lr, momentum)\n",
    "network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                    reduction=\"mean\",\n",
    "                                    smooth_factor=0.1,\n",
    "                                    num_classes=num_classes)\n",
    "\n",
    "loss_net = CustomWithLossCell(network, network_loss)\n",
    "eval_net = CustomWithEvalCell(network)\n",
    "    \n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=3 * step_size, keep_checkpoint_max=100)\n",
    "ckpt_callback = ModelCheckpoint(prefix='vit_b_16', directory='./ViT1', config=ckpt_config)\n",
    "ascend_target = (ms.get_context(\"device_target\") == \"Ascend\")\n",
    "if ascend_target:\n",
    "    model = ms.Model(loss_net, optimizer=network_opt, eval_network=eval_net, metrics={\"acc\"}, amp_level=\"O2\")\n",
    "else:\n",
    "    model = ms.Model(loss_net, optimizer=network_opt, eval_network=eval_net, metrics={\"acc\"}, amp_level=\"O0\")\n",
    "model.train(epoch_size,\n",
    "            train_data,\n",
    "            callbacks=[ckpt_callback, LossMonitor(step_size), TimeMonitor(step_size)]\n",
    "            )\n",
    "result = model.eval(val_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8227d6-e40c-4ffe-b36c-cbb3bd40628c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ma-user/work/mindcon_food_classification-main-99dad61560528d4d507df048402405d3db6bed18'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36974667-ab8b-407a-b7ef-f3cb79ab129b",
   "metadata": {},
   "source": [
    "开始预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdac8156-4823-455a-a900-94e29ff80b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n",
      "8\n",
      "9\n",
      "5\n",
      "1\n",
      "7\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "3\n",
      "7\n",
      "7\n",
      "8\n",
      "4\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "0\n",
      "4\n",
      "6\n",
      "8\n",
      "1\n",
      "4\n",
      "7\n",
      "5\n",
      "2\n",
      "0\n",
      "2\n",
      "7\n",
      "3\n",
      "9\n",
      "6\n",
      "0\n",
      "5\n",
      "7\n",
      "6\n",
      "3\n",
      "4\n",
      "0\n",
      "1\n",
      "1\n",
      "4\n",
      "8\n",
      "9\n",
      "8\n",
      "1\n",
      "3\n",
      "8\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "5\n",
      "8\n",
      "8\n",
      "0\n",
      "5\n",
      "2\n",
      "9\n",
      "6\n",
      "6\n",
      "8\n",
      "0\n",
      "4\n",
      "0\n",
      "5\n",
      "2\n",
      "7\n",
      "2\n",
      "2\n",
      "2\n",
      "7\n",
      "0\n",
      "3\n",
      "6\n",
      "6\n",
      "8\n",
      "3\n",
      "1\n",
      "9\n",
      "5\n",
      "3\n",
      "4\n",
      "0\n",
      "3\n",
      "3\n",
      "4\n",
      "9\n",
      "1\n",
      "2\n",
      "1\n",
      "7\n",
      "1\n",
      "3\n",
      "8\n",
      "1\n",
      "9\n",
      "7\n",
      "1\n",
      "9\n",
      "0\n",
      "7\n",
      "1\n",
      "6\n",
      "4\n",
      "5\n",
      "1\n",
      "0\n",
      "6\n",
      "8\n",
      "2\n",
      "6\n",
      "0\n",
      "2\n",
      "5\n",
      "9\n",
      "4\n",
      "5\n",
      "4\n",
      "1\n",
      "5\n",
      "9\n",
      "8\n",
      "2\n",
      "4\n",
      "0\n",
      "4\n",
      "7\n",
      "9\n",
      "5\n",
      "2\n",
      "6\n",
      "9\n",
      "2\n",
      "5\n",
      "9\n",
      "2\n",
      "1\n",
      "0\n",
      "6\n",
      "0\n",
      "2\n",
      "7\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "5\n",
      "6\n",
      "0\n",
      "1\n",
      "0\n",
      "9\n",
      "7\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "7\n",
      "1\n",
      "8\n",
      "6\n",
      "0\n",
      "9\n",
      "9\n",
      "2\n",
      "4\n",
      "1\n",
      "5\n",
      "0\n",
      "9\n",
      "7\n",
      "8\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "1\n",
      "6\n",
      "6\n",
      "9\n",
      "9\n",
      "8\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "3\n",
      "5\n",
      "3\n",
      "9\n",
      "0\n",
      "0\n",
      "8\n",
      "2\n",
      "3\n",
      "3\n",
      "8\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "8\n",
      "8\n",
      "7\n",
      "5\n",
      "4\n",
      "1\n",
      "7\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "8\n",
      "5\n",
      "2\n",
      "0\n",
      "8\n",
      "2\n",
      "0\n",
      "4\n",
      "7\n",
      "7\n",
      "9\n",
      "0\n",
      "8\n",
      "8\n",
      "8\n",
      "7\n",
      "9\n",
      "2\n",
      "3\n",
      "0\n",
      "9\n",
      "3\n",
      "1\n",
      "9\n",
      "3\n",
      "5\n",
      "0\n",
      "3\n",
      "0\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "6\n",
      "0\n",
      "2\n",
      "8\n",
      "6\n",
      "5\n",
      "7\n",
      "7\n",
      "9\n",
      "5\n",
      "8\n",
      "4\n",
      "1\n",
      "9\n",
      "5\n",
      "4\n",
      "9\n",
      "8\n",
      "7\n",
      "3\n",
      "5\n",
      "6\n",
      "6\n",
      "4\n",
      "8\n",
      "4\n",
      "9\n",
      "5\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "6\n",
      "5\n",
      "2\n",
      "9\n",
      "4\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "8\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "4\n",
      "1\n",
      "2\n",
      "5\n",
      "9\n",
      "5\n",
      "1\n",
      "6\n",
      "7\n",
      "8\n",
      "0\n",
      "6\n",
      "8\n",
      "6\n",
      "1\n",
      "6\n",
      "1\n",
      "6\n",
      "7\n",
      "6\n",
      "2\n",
      "6\n",
      "9\n",
      "8\n",
      "1\n",
      "2\n",
      "2\n",
      "8\n",
      "4\n",
      "5\n",
      "4\n",
      "0\n",
      "4\n",
      "7\n",
      "0\n",
      "3\n",
      "6\n",
      "1\n",
      "7\n",
      "9\n",
      "9\n",
      "0\n",
      "3\n",
      "0\n",
      "7\n",
      "5\n",
      "3\n",
      "2\n",
      "8\n",
      "6\n",
      "8\n",
      "8\n",
      "6\n",
      "8\n",
      "6\n",
      "6\n",
      "2\n",
      "3\n",
      "1\n",
      "9\n",
      "0\n",
      "9\n",
      "3\n",
      "5\n",
      "7\n",
      "1\n",
      "4\n",
      "1\n",
      "3\n",
      "7\n",
      "6\n",
      "2\n",
      "9\n",
      "7\n",
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "0\n",
      "8\n",
      "1\n",
      "0\n",
      "4\n",
      "3\n",
      "7\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "0\n",
      "2\n",
      "4\n",
      "5\n",
      "0\n",
      "2\n",
      "6\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "5\n",
      "3\n",
      "1\n",
      "8\n",
      "5\n",
      "3\n",
      "8\n",
      "3\n",
      "7\n",
      "0\n",
      "9\n",
      "6\n",
      "5\n",
      "2\n",
      "0\n",
      "5\n",
      "7\n",
      "7\n",
      "9\n",
      "2\n",
      "8\n",
      "3\n",
      "6\n",
      "9\n",
      "4\n",
      "0\n",
      "6\n",
      "9\n",
      "6\n",
      "9\n",
      "6\n",
      "4\n",
      "8\n",
      "4\n",
      "6\n",
      "7\n",
      "6\n",
      "9\n",
      "5\n",
      "3\n",
      "7\n",
      "9\n",
      "7\n",
      "4\n",
      "8\n",
      "3\n",
      "9\n",
      "3\n",
      "1\n",
      "5\n",
      "2\n",
      "9\n",
      "4\n",
      "8\n",
      "3\n",
      "3\n",
      "1\n",
      "5\n",
      "5\n",
      "6\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "8\n",
      "4\n",
      "3\n",
      "1\n",
      "8\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "7\n",
      "6\n",
      "4\n",
      "1\n",
      "9\n",
      "7\n",
      "8\n",
      "9\n",
      "3\n",
      "6\n",
      "5\n",
      "9\n",
      "7\n",
      "1\n",
      "[7, 1, 8, 9, 5, 1, 7, 2, 2, 4, 2, 1, 3, 7, 7, 8, 4, 2, 0, 1, 1, 4, 0, 4, 6, 8, 1, 4, 7, 5, 2, 0, 2, 7, 3, 9, 6, 0, 5, 7, 6, 3, 4, 0, 1, 1, 4, 8, 9, 8, 1, 3, 8, 2, 5, 2, 2, 5, 8, 8, 0, 5, 2, 9, 6, 6, 8, 0, 4, 0, 5, 2, 7, 2, 2, 2, 7, 0, 3, 6, 6, 8, 3, 1, 9, 5, 3, 4, 0, 3, 3, 4, 9, 1, 2, 1, 7, 1, 3, 8, 1, 9, 7, 1, 9, 0, 7, 1, 6, 4, 5, 1, 0, 6, 8, 2, 6, 0, 2, 5, 9, 4, 5, 4, 1, 5, 9, 8, 2, 4, 0, 4, 7, 9, 5, 2, 6, 9, 2, 5, 9, 2, 1, 0, 6, 0, 2, 7, 0, 1, 2, 0, 2, 1, 0, 5, 6, 0, 1, 0, 9, 7, 2, 3, 3, 0, 4, 3, 4, 4, 7, 1, 8, 6, 0, 9, 9, 2, 4, 1, 5, 0, 9, 7, 8, 5, 4, 4, 5, 4, 1, 6, 6, 9, 9, 8, 5, 5, 6, 6, 3, 5, 3, 9, 0, 0, 8, 2, 3, 3, 8, 4, 3, 4, 6, 8, 8, 7, 5, 4, 1, 7, 9, 5, 5, 5, 4, 8, 5, 2, 0, 8, 2, 0, 4, 7, 7, 9, 0, 8, 8, 8, 7, 9, 2, 3, 0, 9, 3, 1, 9, 3, 5, 0, 3, 0, 9, 8, 7, 6, 3, 1, 3, 3, 4, 2, 3, 6, 0, 2, 8, 6, 5, 7, 7, 9, 5, 8, 4, 1, 9, 5, 4, 9, 8, 7, 3, 5, 6, 6, 4, 8, 4, 9, 5, 0, 0, 3, 2, 6, 5, 2, 9, 4, 0, 1, 3, 0, 8, 6, 7, 8, 9, 4, 1, 2, 5, 9, 5, 1, 6, 7, 8, 0, 6, 8, 6, 1, 6, 1, 6, 7, 6, 2, 6, 9, 8, 1, 2, 2, 8, 4, 5, 4, 0, 4, 7, 0, 3, 6, 1, 7, 9, 9, 0, 3, 0, 7, 5, 3, 2, 8, 6, 8, 8, 6, 8, 6, 6, 2, 3, 1, 9, 0, 9, 3, 5, 7, 1, 4, 1, 3, 7, 6, 2, 9, 7, 1, 2, 4, 3, 0, 8, 1, 0, 4, 3, 7, 2, 2, 2, 4, 0, 2, 4, 5, 0, 2, 6, 2, 5, 0, 0, 5, 3, 1, 8, 5, 3, 8, 3, 7, 0, 9, 6, 5, 2, 0, 5, 7, 7, 9, 2, 8, 3, 6, 9, 4, 0, 6, 9, 6, 9, 6, 4, 8, 4, 6, 7, 6, 9, 5, 3, 7, 9, 7, 4, 8, 3, 9, 3, 1, 5, 2, 9, 4, 8, 3, 3, 1, 5, 5, 6, 7, 4, 8, 7, 8, 4, 3, 1, 8, 5, 2, 5, 5, 7, 6, 4, 1, 9, 7, 8, 9, 3, 6, 5, 9, 7, 1]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor\n",
    "\n",
    "# from vit import ViT\n",
    "\n",
    "def process_image(path):\n",
    "\n",
    "    image_list = []\n",
    "\n",
    "    mean = np.array([0.485 * 255, 0.456 * 255, 0.406 * 255])\n",
    "    std = np.array([0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    files = sorted(files, key=lambda x:(x.split('.')[0]))\n",
    "    \n",
    "    for file in files:\n",
    "        image = Image.open(path + file).convert(\"RGB\")\n",
    "        image = image.resize((224, 224))\n",
    "        # plt.imshow(image)\n",
    "        image = (image - mean) / std\n",
    "        image = image.astype(np.float32)\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image_list.append(image)\n",
    "    return image_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ms.set_context(device_target=\"CPU\")\n",
    "# ms.set_context(device_target=\"Ascend\")\n",
    "images = process_image(\"./test/\")\n",
    "\n",
    "network = ViT(num_classes=10)\n",
    "vit_path = \"./ViT1/vit_b_16-48_126.ckpt\"\n",
    "param_dict = ms.load_checkpoint(vit_path)\n",
    "ms.load_param_into_net(network, param_dict)\n",
    "\n",
    "model = ms.Model(network)\n",
    "\n",
    "results = []\n",
    "for image in images:\n",
    "    pre = model.predict(Tensor(image,ms.float32))\n",
    "    result = np.argmax(pre)\n",
    "    results.append(result)\n",
    "    print(result)\n",
    "        \n",
    "with open(\"food0113.txt\",\"w\") as f:\n",
    "    for result in results:\n",
    "        f.write(str(result))\n",
    "        f.write('\\n')\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15129f10-d22f-4c3f-b1e0-5f5357f8c466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
